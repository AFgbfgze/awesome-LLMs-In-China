# 开源开放的大模型

旨在记录全球开源开放大模型发展情况，欢迎提供
- *线索*
- *材料*
- *PR*
- *Issue*


## 基础大模型
|序号|名称|参数规模|数据规模|说明|
|:-|:-|:-|:-|:-|
|1|[LLaMA](Open-LLMs/llama.md)|7B,13B,30B,65B|1.4T|Meta，代码开源，模型“泄露”,不可商用，[详细介绍](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)|
|2|[LLaMA-2](Open-LLMs/llama2.md)|7B,13B,70B|2T|可商用|
|3|[BLOOM](Open-LLMs/bloom.md)|3B,7.1B,176B|366B|可商用，最为宽松，[详细介绍](https://mp.weixin.qq.com/s/ia-yrmXbnlooRA3K1hoTwQ)|
|4|GALACTICA|6.7B,30B,120B|106B|开放的科学文本和数据|
|5|[Falcon](Open-LLMs/falcon.md)|7B,40B|1T|数据集[ RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)|
|6|MOSS-moon|16B|700B|6.67x1022 FLOPs|
|7|ChatGLM2|6B|1.4T||
|8|StableLM|3B,7B|800B||
|9|[baichuan](Open-LLMs/baichuan.md)|7B,13B|1.4T|开放，商用需授权，7B-1.2T，13B-1.4T|
|10|Aquila|7B||悟道·天鹰|
|11|RedPajama-INCITE|3B,7B|1T||
|12|GPT-NeoX|20B|3.15M|800GB的[The Pile](https://arxiv.org/abs/2101.00027)数据集|
|13|OpenLLaMA|3B,7B,13B|1T||
|14|MPT|7B,30B|1T|
|15|Pythia|2.8B,6.9B,12B|300B||
|16|XGen|7B|1.5T||
|17|OPT|6.7B,13B,30B,66B,175B|180B||
|18|Qwen|7B|2.2T||
|19|XVERSE|13B|1.4T||
|20|Prithvi|||IBM+NASA,地理空间，100M（图片）|

## 非基础大模型
- WizardLM
- Alpaca
- Vicuna
- Guanaco



## 模型架构

- [GPTQ](https://github.com/IST-DASLab/gptq)


